{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OmniField for N-MNIST Classification\n",
    "\n",
    "This notebook implements OmniField for Neuromorphic MNIST (N-MNIST) classification.\n",
    "\n",
    "**N-MNIST** is a spiking/event-based version of MNIST captured using a Dynamic Vision Sensor (DVS).\n",
    "Events are represented as `(x, y, t, polarity)` - perfectly suited for neural field approaches.\n",
    "\n",
    "## Two Classification Approaches:\n",
    "1. **End-to-End Classification**: Direct classification from events using OmniField\n",
    "2. **Reconstruction + Fine-tuning**: First reconstruct the image, then classify\n",
    "\n",
    "## Architecture:\n",
    "- Small base OmniField\n",
    "- Deep ICMR (6 layers) for cross-modal refinement\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install tonic torch torchvision matplotlib numpy seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from functools import wraps\n",
    "from math import log\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "# Try to import tonic for N-MNIST\n",
    "try:\n",
    "    import tonic\n",
    "    from tonic import transforms as tonic_transforms\n",
    "    TONIC_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Tonic not available. Install with: pip install tonic\")\n",
    "    TONIC_AVAILABLE = False\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore N-MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load N-MNIST dataset using tonic\n",
    "if TONIC_AVAILABLE:\n",
    "    # Download and load N-MNIST\n",
    "    nmnist_train = tonic.datasets.NMNIST(save_to='./data', train=True)\n",
    "    nmnist_test = tonic.datasets.NMNIST(save_to='./data', train=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(nmnist_train)}\")\n",
    "    print(f\"Test samples: {len(nmnist_test)}\")\n",
    "    print(f\"Sensor size: {nmnist_train.sensor_size}\")\n",
    "else:\n",
    "    print(\"Please install tonic to load N-MNIST dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a single sample\n",
    "if TONIC_AVAILABLE:\n",
    "    events, label = nmnist_train[0]\n",
    "    \n",
    "    print(f\"\\n=== Sample 0 (Label: {label}) ===\")\n",
    "    print(f\"Events shape: {events.shape}\")\n",
    "    print(f\"Event dtype: {events.dtype}\")\n",
    "    print(f\"\\nEvent structure (first 5 events):\")\n",
    "    print(f\"  x: {events['x'][:5]}\")\n",
    "    print(f\"  y: {events['y'][:5]}\")\n",
    "    print(f\"  t: {events['t'][:5]} (microseconds)\")\n",
    "    print(f\"  p: {events['p'][:5]} (polarity: 0=OFF, 1=ON)\")\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Total events: {len(events)}\")\n",
    "    print(f\"  Time range: {events['t'].min()} - {events['t'].max()} μs\")\n",
    "    print(f\"  Duration: {(events['t'].max() - events['t'].min()) / 1000:.2f} ms\")\n",
    "    print(f\"  X range: {events['x'].min()} - {events['x'].max()}\")\n",
    "    print(f\"  Y range: {events['y'].min()} - {events['y'].max()}\")\n",
    "    print(f\"  ON events: {(events['p'] == 1).sum()} ({100*(events['p'] == 1).mean():.1f}%)\")\n",
    "    print(f\"  OFF events: {(events['p'] == 0).sum()} ({100*(events['p'] == 0).mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Statistics and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dataset_statistics(dataset, num_samples=1000):\n",
    "    \"\"\"Compute statistics over N-MNIST dataset.\"\"\"\n",
    "    stats = {\n",
    "        'num_events': [],\n",
    "        'duration_ms': [],\n",
    "        'on_ratio': [],\n",
    "        'labels': []\n",
    "    }\n",
    "    \n",
    "    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    \n",
    "    for idx in tqdm(indices, desc=\"Computing statistics\"):\n",
    "        events, label = dataset[idx]\n",
    "        stats['num_events'].append(len(events))\n",
    "        stats['duration_ms'].append((events['t'].max() - events['t'].min()) / 1000)\n",
    "        stats['on_ratio'].append((events['p'] == 1).mean())\n",
    "        stats['labels'].append(label)\n",
    "    \n",
    "    return {k: np.array(v) for k, v in stats.items()}\n",
    "\n",
    "if TONIC_AVAILABLE:\n",
    "    stats = compute_dataset_statistics(nmnist_train, num_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TONIC_AVAILABLE:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 1. Distribution of number of events\n",
    "    ax = axes[0, 0]\n",
    "    ax.hist(stats['num_events'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(np.mean(stats['num_events']), color='red', linestyle='--', label=f'Mean: {np.mean(stats[\"num_events\"]):.0f}')\n",
    "    ax.set_xlabel('Number of Events')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Distribution of Events per Sample')\n",
    "    ax.legend()\n",
    "    \n",
    "    # 2. Distribution of event duration\n",
    "    ax = axes[0, 1]\n",
    "    ax.hist(stats['duration_ms'], bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "    ax.axvline(np.mean(stats['duration_ms']), color='red', linestyle='--', label=f'Mean: {np.mean(stats[\"duration_ms\"]):.1f} ms')\n",
    "    ax.set_xlabel('Duration (ms)')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Distribution of Event Duration')\n",
    "    ax.legend()\n",
    "    \n",
    "    # 3. ON/OFF polarity ratio\n",
    "    ax = axes[1, 0]\n",
    "    ax.hist(stats['on_ratio'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "    ax.axvline(np.mean(stats['on_ratio']), color='red', linestyle='--', label=f'Mean: {np.mean(stats[\"on_ratio\"]):.2f}')\n",
    "    ax.set_xlabel('ON Event Ratio')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Distribution of ON Polarity Ratio')\n",
    "    ax.legend()\n",
    "    \n",
    "    # 4. Class distribution\n",
    "    ax = axes[1, 1]\n",
    "    label_counts = Counter(stats['labels'])\n",
    "    ax.bar(range(10), [label_counts[i] for i in range(10)], edgecolor='black', alpha=0.7, color='purple')\n",
    "    ax.set_xlabel('Digit Class')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Class Distribution (Sampled)')\n",
    "    ax.set_xticks(range(10))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('nmnist_statistics.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n=== N-MNIST Dataset Statistics ===\")\n",
    "    print(f\"Events per sample: {np.mean(stats['num_events']):.0f} ± {np.std(stats['num_events']):.0f}\")\n",
    "    print(f\"Duration: {np.mean(stats['duration_ms']):.1f} ± {np.std(stats['duration_ms']):.1f} ms\")\n",
    "    print(f\"ON ratio: {np.mean(stats['on_ratio']):.3f} ± {np.std(stats['on_ratio']):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_events(events, label, ax=None, title=None):\n",
    "    \"\"\"Visualize events as a 2D histogram (accumulated frame).\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    \n",
    "    # Create accumulated frame\n",
    "    frame = np.zeros((34, 34))\n",
    "    for i in range(len(events)):\n",
    "        x, y, p = events['x'][i], events['y'][i], events['p'][i]\n",
    "        frame[y, x] += (2 * p - 1)  # +1 for ON, -1 for OFF\n",
    "    \n",
    "    # Normalize\n",
    "    frame = np.clip(frame, -50, 50)\n",
    "    \n",
    "    im = ax.imshow(frame, cmap='RdBu_r', vmin=-50, vmax=50)\n",
    "    ax.set_title(title or f'Label: {label}')\n",
    "    ax.axis('off')\n",
    "    return im\n",
    "\n",
    "def visualize_events_3d(events, label, ax=None):\n",
    "    \"\"\"Visualize events in 3D (x, y, t).\"\"\"\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Subsample for visualization\n",
    "    n = min(2000, len(events))\n",
    "    idx = np.random.choice(len(events), n, replace=False)\n",
    "    \n",
    "    x = events['x'][idx]\n",
    "    y = events['y'][idx]\n",
    "    t = (events['t'][idx] - events['t'].min()) / 1000  # Convert to ms\n",
    "    p = events['p'][idx]\n",
    "    \n",
    "    colors = ['blue' if pol == 0 else 'red' for pol in p]\n",
    "    ax.scatter(x, y, t, c=colors, s=1, alpha=0.5)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Time (ms)')\n",
    "    ax.set_title(f'Label: {label}')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TONIC_AVAILABLE:\n",
    "    # Visualize samples from each class\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    \n",
    "    # Find one sample per class\n",
    "    class_samples = {}\n",
    "    for idx in range(len(nmnist_train)):\n",
    "        events, label = nmnist_train[idx]\n",
    "        if label not in class_samples:\n",
    "            class_samples[label] = (events, label)\n",
    "        if len(class_samples) == 10:\n",
    "            break\n",
    "    \n",
    "    for i in range(10):\n",
    "        events, label = class_samples[i]\n",
    "        ax = axes[i // 5, i % 5]\n",
    "        visualize_events(events, label, ax=ax, title=f'Digit: {label}')\n",
    "    \n",
    "    plt.suptitle('N-MNIST Samples (Accumulated Events)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('nmnist_samples.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TONIC_AVAILABLE:\n",
    "    # 3D visualization of a few samples\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, digit in enumerate([0, 3, 8]):\n",
    "        events, label = class_samples[digit]\n",
    "        ax = fig.add_subplot(1, 3, i+1, projection='3d')\n",
    "        visualize_events_3d(events, label, ax=ax)\n",
    "    \n",
    "    plt.suptitle('N-MNIST Events in 3D (x, y, time)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('nmnist_3d.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. OmniField Architecture for N-MNIST\n",
    "\n",
    "Key adaptations:\n",
    "- **Input**: Sparse events (x, y, t, polarity) → Neural field\n",
    "- **Small base model** with **6 ICMR layers**\n",
    "- **Two output heads**: Classification and Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Helper Functions and Modules\n",
    "# ============================================================\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs['context']\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context=normed_context)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim=-1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, query_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        h = self.heads\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
    "        sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        out = torch.einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Gaussian Fourier Features (GFF)\n",
    "# ============================================================\n",
    "\n",
    "class GaussianFourierFeatures(nn.Module):\n",
    "    \"\"\"Gaussian Fourier Features for positional encoding.\"\"\"\n",
    "    def __init__(self, in_features, mapping_size, scale=10.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.mapping_size = mapping_size\n",
    "        # B ~ N(0, scale^2)\n",
    "        self.register_buffer('B', torch.randn((in_features, mapping_size)) * scale)\n",
    "\n",
    "    def forward(self, coords):\n",
    "        # coords: [..., in_features]\n",
    "        projections = coords @ self.B  # [..., mapping_size]\n",
    "        fourier_feats = torch.cat([torch.sin(2 * np.pi * projections), \n",
    "                                   torch.cos(2 * np.pi * projections)], dim=-1)\n",
    "        return fourier_feats  # [..., 2 * mapping_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Sinusoidal Initialization for Learnable Queries\n",
    "# ============================================================\n",
    "\n",
    "def get_sinusoidal_embeddings(n, d):\n",
    "    \"\"\"Generate sinusoidal positional embeddings.\"\"\"\n",
    "    assert d % 2 == 0, \"Dimension must be even\"\n",
    "    position = torch.arange(n, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d, 2).float() * -(log(10000.0) / d))\n",
    "    pe = torch.zeros(n, d)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ICMR Block (Iterative Cross-Modal Refinement)\n",
    "# ============================================================\n",
    "\n",
    "class ICMRBlock(nn.Module):\n",
    "    \"\"\"Single ICMR block with cross-attention and self-attention.\"\"\"\n",
    "    def __init__(self, dim, num_latents, input_dim, heads=4, dim_head=32, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.latents = nn.Parameter(get_sinusoidal_embeddings(num_latents, dim))\n",
    "        \n",
    "        # Cross-attention: latents attend to input\n",
    "        self.cross_attn = PreNorm(\n",
    "            dim, \n",
    "            Attention(dim, input_dim, heads=heads, dim_head=dim_head, dropout=dropout),\n",
    "            context_dim=input_dim\n",
    "        )\n",
    "        \n",
    "        # Self-attention among latents\n",
    "        self.self_attn = PreNorm(\n",
    "            dim,\n",
    "            Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)\n",
    "        )\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ff = PreNorm(dim, FeedForward(dim, dropout=dropout))\n",
    "        \n",
    "        # Global feature projection (for ICMR)\n",
    "        self.global_proj = nn.Linear(dim, dim)\n",
    "    \n",
    "    def forward(self, context, global_feature=None, mask=None):\n",
    "        b = context.size(0)\n",
    "        latents = repeat(self.latents, 'n d -> b n d', b=b)\n",
    "        \n",
    "        # Add global feature if provided (ICMR)\n",
    "        if global_feature is not None:\n",
    "            global_bias = self.global_proj(global_feature).unsqueeze(1)\n",
    "            latents = latents + global_bias\n",
    "        \n",
    "        # Cross-attention\n",
    "        latents = self.cross_attn(latents, context=context, mask=mask) + latents\n",
    "        \n",
    "        # Self-attention\n",
    "        latents = self.self_attn(latents) + latents\n",
    "        \n",
    "        # Feed-forward\n",
    "        latents = self.ff(latents) + latents\n",
    "        \n",
    "        return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OmniField for N-MNIST (Small base, 6 ICMR layers)\n",
    "# ============================================================\n",
    "\n",
    "class OmniFieldNMNIST(nn.Module):\n",
    "    \"\"\"\n",
    "    OmniField adapted for N-MNIST classification and reconstruction.\n",
    "    \n",
    "    Architecture:\n",
    "    - Small base model (dim=64)\n",
    "    - 6 ICMR layers for deep cross-modal refinement\n",
    "    - Dual heads: classification and reconstruction\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=64,                    # Small base dimension\n",
    "        num_latents=64,            # Number of latent tokens\n",
    "        num_icmr_layers=6,         # Deep ICMR (6 layers)\n",
    "        heads=4,\n",
    "        dim_head=16,\n",
    "        num_classes=10,\n",
    "        spatial_size=34,           # N-MNIST is 34x34\n",
    "        dropout=0.1,\n",
    "        gff_scale=10.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_latents = num_latents\n",
    "        self.num_icmr_layers = num_icmr_layers\n",
    "        self.spatial_size = spatial_size\n",
    "        \n",
    "        # ========== Encoder ==========\n",
    "        # Spatial positional encoding (x, y)\n",
    "        self.spatial_enc = GaussianFourierFeatures(2, 16, scale=gff_scale)\n",
    "        # Temporal encoding (t)\n",
    "        self.temporal_enc = GaussianFourierFeatures(1, 8, scale=gff_scale)\n",
    "        \n",
    "        # Input projection: polarity (1) + spatial_enc (32) + temporal_enc (16) -> dim\n",
    "        input_dim = 1 + 32 + 16  # 49\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "        \n",
    "        # ========== ICMR Blocks (6 layers) ==========\n",
    "        self.icmr_blocks = nn.ModuleList([\n",
    "            ICMRBlock(\n",
    "                dim=dim,\n",
    "                num_latents=num_latents,\n",
    "                input_dim=dim,\n",
    "                heads=heads,\n",
    "                dim_head=dim_head,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(num_icmr_layers)\n",
    "        ])\n",
    "        \n",
    "        # ========== Classification Head ==========\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        # ========== Reconstruction Head ==========\n",
    "        # Query encoding for reconstruction\n",
    "        self.query_enc = GaussianFourierFeatures(2, 16, scale=gff_scale)\n",
    "        self.query_proj = nn.Linear(32, dim)\n",
    "        \n",
    "        # Decoder cross-attention\n",
    "        self.decoder_cross_attn = PreNorm(\n",
    "            dim,\n",
    "            Attention(dim, dim, heads=heads, dim_head=dim_head, dropout=dropout),\n",
    "            context_dim=dim\n",
    "        )\n",
    "        self.decoder_ff = PreNorm(dim, FeedForward(dim, dropout=dropout))\n",
    "        \n",
    "        # Output: predict intensity at query locations\n",
    "        self.recon_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, 1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def encode_events(self, x, y, t, p):\n",
    "        \"\"\"\n",
    "        Encode sparse events into dense features.\n",
    "        \n",
    "        Args:\n",
    "            x: [B, N] x-coordinates (normalized to [0, 1])\n",
    "            y: [B, N] y-coordinates (normalized to [0, 1])\n",
    "            t: [B, N] timestamps (normalized to [0, 1])\n",
    "            p: [B, N] polarity (-1 or 1)\n",
    "        \n",
    "        Returns:\n",
    "            features: [B, N, dim]\n",
    "        \"\"\"\n",
    "        B, N = x.shape\n",
    "        \n",
    "        # Spatial encoding\n",
    "        spatial_coords = torch.stack([x, y], dim=-1)  # [B, N, 2]\n",
    "        spatial_feat = self.spatial_enc(spatial_coords)  # [B, N, 32]\n",
    "        \n",
    "        # Temporal encoding\n",
    "        temporal_feat = self.temporal_enc(t.unsqueeze(-1))  # [B, N, 16]\n",
    "        \n",
    "        # Combine with polarity\n",
    "        p_feat = p.unsqueeze(-1)  # [B, N, 1]\n",
    "        \n",
    "        combined = torch.cat([p_feat, spatial_feat, temporal_feat], dim=-1)  # [B, N, 49]\n",
    "        features = self.input_proj(combined)  # [B, N, dim]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def forward_encoder(self, x, y, t, p, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through encoder and ICMR blocks.\n",
    "        \n",
    "        Returns:\n",
    "            latents: [B, num_latents, dim]\n",
    "            global_feature: [B, dim]\n",
    "        \"\"\"\n",
    "        # Encode events\n",
    "        features = self.encode_events(x, y, t, p)  # [B, N, dim]\n",
    "        \n",
    "        # ICMR: iterative refinement with global feature\n",
    "        global_feature = None\n",
    "        latents = None\n",
    "        \n",
    "        for block in self.icmr_blocks:\n",
    "            latents = block(features, global_feature=global_feature, mask=mask)\n",
    "            # Update global feature (mean pooling)\n",
    "            global_feature = latents.mean(dim=1)  # [B, dim]\n",
    "        \n",
    "        return latents, global_feature\n",
    "    \n",
    "    def forward_classification(self, latents, global_feature):\n",
    "        \"\"\"\n",
    "        Classification head.\n",
    "        \n",
    "        Args:\n",
    "            latents: [B, num_latents, dim]\n",
    "            global_feature: [B, dim]\n",
    "        \n",
    "        Returns:\n",
    "            logits: [B, num_classes]\n",
    "        \"\"\"\n",
    "        # Use global feature for classification\n",
    "        logits = self.cls_head(global_feature)\n",
    "        return logits\n",
    "    \n",
    "    def forward_reconstruction(self, latents, query_coords):\n",
    "        \"\"\"\n",
    "        Reconstruction head.\n",
    "        \n",
    "        Args:\n",
    "            latents: [B, num_latents, dim]\n",
    "            query_coords: [B, Q, 2] query coordinates (x, y) normalized to [0, 1]\n",
    "        \n",
    "        Returns:\n",
    "            recon: [B, Q, 1] reconstructed intensities\n",
    "        \"\"\"\n",
    "        # Encode query positions\n",
    "        query_feat = self.query_enc(query_coords)  # [B, Q, 32]\n",
    "        query_feat = self.query_proj(query_feat)   # [B, Q, dim]\n",
    "        \n",
    "        # Cross-attention: queries attend to latents\n",
    "        decoded = self.decoder_cross_attn(query_feat, context=latents) + query_feat\n",
    "        decoded = self.decoder_ff(decoded) + decoded\n",
    "        \n",
    "        # Output intensities\n",
    "        recon = self.recon_head(decoded)\n",
    "        return recon\n",
    "    \n",
    "    def forward(self, x, y, t, p, mask=None, query_coords=None, return_recon=False):\n",
    "        \"\"\"\n",
    "        Full forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x, y, t, p: Event coordinates and polarity\n",
    "            mask: Optional attention mask\n",
    "            query_coords: Query coordinates for reconstruction\n",
    "            return_recon: Whether to return reconstruction\n",
    "        \n",
    "        Returns:\n",
    "            logits: Classification logits\n",
    "            recon: (optional) Reconstruction output\n",
    "        \"\"\"\n",
    "        latents, global_feature = self.forward_encoder(x, y, t, p, mask)\n",
    "        logits = self.forward_classification(latents, global_feature)\n",
    "        \n",
    "        if return_recon and query_coords is not None:\n",
    "            recon = self.forward_reconstruction(latents, query_coords)\n",
    "            return logits, recon\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model = OmniFieldNMNIST(\n",
    "    dim=64,\n",
    "    num_latents=64,\n",
    "    num_icmr_layers=6,\n",
    "    heads=4,\n",
    "    dim_head=16,\n",
    "    num_classes=10,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\n",
    "\n",
    "# Test forward pass\n",
    "B, N = 4, 1000\n",
    "test_x = torch.rand(B, N).to(DEVICE)\n",
    "test_y = torch.rand(B, N).to(DEVICE)\n",
    "test_t = torch.rand(B, N).to(DEVICE)\n",
    "test_p = torch.randint(0, 2, (B, N)).float().to(DEVICE) * 2 - 1  # -1 or 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(test_x, test_y, test_t, test_p)\n",
    "    print(f\"Output shape: {logits.shape}\")\n",
    "    \n",
    "    # With reconstruction\n",
    "    Q = 34 * 34\n",
    "    query_coords = torch.stack(torch.meshgrid(\n",
    "        torch.linspace(0, 1, 34),\n",
    "        torch.linspace(0, 1, 34),\n",
    "        indexing='ij'\n",
    "    ), dim=-1).reshape(1, Q, 2).expand(B, -1, -1).to(DEVICE)\n",
    "    \n",
    "    logits, recon = model(test_x, test_y, test_t, test_p, query_coords=query_coords, return_recon=True)\n",
    "    print(f\"Reconstruction shape: {recon.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Wrapper for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    N-MNIST dataset wrapper for OmniField.\n",
    "    \n",
    "    Converts raw events to normalized tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, tonic_dataset, max_events=2048, spatial_size=34, augment=False):\n",
    "        self.dataset = tonic_dataset\n",
    "        self.max_events = max_events\n",
    "        self.spatial_size = spatial_size\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        events, label = self.dataset[idx]\n",
    "        \n",
    "        # Extract event components\n",
    "        x = events['x'].astype(np.float32)\n",
    "        y = events['y'].astype(np.float32)\n",
    "        t = events['t'].astype(np.float32)\n",
    "        p = events['p'].astype(np.float32)\n",
    "        \n",
    "        # Normalize coordinates to [0, 1]\n",
    "        x = x / (self.spatial_size - 1)\n",
    "        y = y / (self.spatial_size - 1)\n",
    "        \n",
    "        # Normalize time to [0, 1]\n",
    "        t_min, t_max = t.min(), t.max()\n",
    "        if t_max > t_min:\n",
    "            t = (t - t_min) / (t_max - t_min)\n",
    "        else:\n",
    "            t = np.zeros_like(t)\n",
    "        \n",
    "        # Convert polarity to -1/+1\n",
    "        p = p * 2 - 1\n",
    "        \n",
    "        # Data augmentation\n",
    "        if self.augment:\n",
    "            # Random horizontal flip\n",
    "            if np.random.rand() > 0.5:\n",
    "                x = 1.0 - x\n",
    "            # Random time reversal\n",
    "            if np.random.rand() > 0.5:\n",
    "                t = 1.0 - t\n",
    "                p = -p\n",
    "        \n",
    "        # Subsample or pad to max_events\n",
    "        n_events = len(x)\n",
    "        if n_events > self.max_events:\n",
    "            # Random subsample\n",
    "            idx_sub = np.random.choice(n_events, self.max_events, replace=False)\n",
    "            idx_sub = np.sort(idx_sub)\n",
    "            x, y, t, p = x[idx_sub], y[idx_sub], t[idx_sub], p[idx_sub]\n",
    "            mask = np.ones(self.max_events, dtype=np.float32)\n",
    "        else:\n",
    "            # Pad with zeros\n",
    "            pad_len = self.max_events - n_events\n",
    "            x = np.pad(x, (0, pad_len), constant_values=0)\n",
    "            y = np.pad(y, (0, pad_len), constant_values=0)\n",
    "            t = np.pad(t, (0, pad_len), constant_values=0)\n",
    "            p = np.pad(p, (0, pad_len), constant_values=0)\n",
    "            mask = np.concatenate([np.ones(n_events), np.zeros(pad_len)]).astype(np.float32)\n",
    "        \n",
    "        # Create target image for reconstruction (accumulated events)\n",
    "        target_img = np.zeros((self.spatial_size, self.spatial_size), dtype=np.float32)\n",
    "        for i in range(len(events)):\n",
    "            ex, ey, ep = events['x'][i], events['y'][i], events['p'][i]\n",
    "            target_img[ey, ex] += (2 * ep - 1)\n",
    "        # Normalize target\n",
    "        target_img = np.clip(target_img / 50.0, -1, 1)  # Normalize to [-1, 1]\n",
    "        \n",
    "        return {\n",
    "            'x': torch.from_numpy(x),\n",
    "            'y': torch.from_numpy(y),\n",
    "            't': torch.from_numpy(t),\n",
    "            'p': torch.from_numpy(p),\n",
    "            'mask': torch.from_numpy(mask),\n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'target_img': torch.from_numpy(target_img)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function.\"\"\"\n",
    "    return {\n",
    "        'x': torch.stack([b['x'] for b in batch]),\n",
    "        'y': torch.stack([b['y'] for b in batch]),\n",
    "        't': torch.stack([b['t'] for b in batch]),\n",
    "        'p': torch.stack([b['p'] for b in batch]),\n",
    "        'mask': torch.stack([b['mask'] for b in batch]),\n",
    "        'label': torch.stack([b['label'] for b in batch]),\n",
    "        'target_img': torch.stack([b['target_img'] for b in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TONIC_AVAILABLE:\n",
    "    # Create datasets\n",
    "    train_dataset = NMNISTDataset(nmnist_train, max_events=2048, augment=True)\n",
    "    test_dataset = NMNISTDataset(nmnist_test, max_events=2048, augment=False)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=64, \n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=64, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Test a batch\n",
    "    batch = next(iter(train_loader))\n",
    "    print(\"Batch shapes:\")\n",
    "    for k, v in batch.items():\n",
    "        print(f\"  {k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_grid(batch_size, spatial_size=34, device='cpu'):\n",
    "    \"\"\"Create query grid for reconstruction.\"\"\"\n",
    "    coords = torch.stack(torch.meshgrid(\n",
    "        torch.linspace(0, 1, spatial_size),\n",
    "        torch.linspace(0, 1, spatial_size),\n",
    "        indexing='ij'\n",
    "    ), dim=-1)  # [H, W, 2]\n",
    "    coords = coords.reshape(1, spatial_size * spatial_size, 2)  # [1, H*W, 2]\n",
    "    coords = coords.expand(batch_size, -1, -1).to(device)  # [B, H*W, 2]\n",
    "    return coords\n",
    "\n",
    "\n",
    "def train_epoch_e2e(model, loader, optimizer, device, epoch):\n",
    "    \"\"\"Train one epoch - End-to-End classification.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [E2E]\")\n",
    "    for batch in pbar:\n",
    "        x = batch['x'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "        t = batch['t'].to(device)\n",
    "        p = batch['p'].to(device)\n",
    "        mask = batch['mask'].to(device).bool()\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(x, y, t, p, mask=mask)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total\n",
    "\n",
    "\n",
    "def train_epoch_recon(model, loader, optimizer, device, epoch, recon_weight=1.0, cls_weight=0.1):\n",
    "    \"\"\"Train one epoch - Reconstruction + Classification.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_cls_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Recon]\")\n",
    "    for batch in pbar:\n",
    "        x = batch['x'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "        t = batch['t'].to(device)\n",
    "        p = batch['p'].to(device)\n",
    "        mask = batch['mask'].to(device).bool()\n",
    "        labels = batch['label'].to(device)\n",
    "        target_img = batch['target_img'].to(device)  # [B, H, W]\n",
    "        \n",
    "        B = x.size(0)\n",
    "        query_coords = create_query_grid(B, spatial_size=34, device=device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits, recon = model(x, y, t, p, mask=mask, query_coords=query_coords, return_recon=True)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        recon = recon.reshape(B, 34, 34)\n",
    "        recon_loss = F.mse_loss(recon, target_img)\n",
    "        \n",
    "        # Classification loss\n",
    "        cls_loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = recon_weight * recon_loss + cls_weight * cls_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_cls_loss += cls_loss.item()\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'recon': f'{recon_loss.item():.4f}',\n",
    "            'acc': f'{100*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return (\n",
    "        total_loss / len(loader),\n",
    "        total_recon_loss / len(loader),\n",
    "        total_cls_loss / len(loader),\n",
    "        100 * correct / total\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, return_recon=False):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    recon_samples = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "        x = batch['x'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "        t = batch['t'].to(device)\n",
    "        p = batch['p'].to(device)\n",
    "        mask = batch['mask'].to(device).bool()\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        B = x.size(0)\n",
    "        \n",
    "        if return_recon:\n",
    "            query_coords = create_query_grid(B, spatial_size=34, device=device)\n",
    "            logits, recon = model(x, y, t, p, mask=mask, query_coords=query_coords, return_recon=True)\n",
    "            if len(recon_samples) < 10:\n",
    "                recon_samples.append({\n",
    "                    'recon': recon.cpu(),\n",
    "                    'target': batch['target_img'],\n",
    "                    'label': batch['label']\n",
    "                })\n",
    "        else:\n",
    "            logits = model(x, y, t, p, mask=mask)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    \n",
    "    if return_recon:\n",
    "        return avg_loss, accuracy, all_preds, all_labels, recon_samples\n",
    "    return avg_loss, accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Approach 1: End-to-End Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model for E2E classification\n",
    "model_e2e = OmniFieldNMNIST(\n",
    "    dim=64,\n",
    "    num_latents=64,\n",
    "    num_icmr_layers=6,\n",
    "    heads=4,\n",
    "    dim_head=16,\n",
    "    num_classes=10,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"E2E Model parameters: {sum(p.numel() for p in model_e2e.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS_E2E = 30\n",
    "LR = 1e-3\n",
    "\n",
    "optimizer_e2e = AdamW(model_e2e.parameters(), lr=LR, weight_decay=0.01)\n",
    "scheduler_e2e = CosineAnnealingLR(optimizer_e2e, T_max=NUM_EPOCHS_E2E, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TONIC_AVAILABLE:\n",
    "    # Training loop - E2E\n",
    "    history_e2e = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(1, NUM_EPOCHS_E2E + 1):\n",
    "        train_loss, train_acc = train_epoch_e2e(model_e2e, train_loader, optimizer_e2e, DEVICE, epoch)\n",
    "        val_loss, val_acc, _, _ = evaluate(model_e2e, test_loader, DEVICE)\n",
    "        scheduler_e2e.step()\n",
    "        \n",
    "        history_e2e['train_loss'].append(train_loss)\n",
    "        history_e2e['train_acc'].append(train_acc)\n",
    "        history_e2e['val_loss'].append(val_loss)\n",
    "        history_e2e['val_acc'].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model_e2e.state_dict(), 'omnifield_nmnist_e2e_best.pt')\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, \"\n",
    "              f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nBest E2E Validation Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TONIC_AVAILABLE and len(history_e2e['train_loss']) > 0:\n",
    "    # Plot E2E training curves\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].plot(history_e2e['train_loss'], label='Train')\n",
    "    axes[0].plot(history_e2e['val_loss'], label='Val')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('E2E Classification Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    axes[1].plot(history_e2e['train_acc'], label='Train')\n",
    "    axes[1].plot(history_e2e['val_acc'], label='Val')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('E2E Classification Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('e2e_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Approach 2: Reconstruction + Fine-tuning Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model for reconstruction + classification\n",
    "model_recon = OmniFieldNMNIST(\n",
    "    dim=64,\n",
    "    num_latents=64,\n",
    "    num_icmr_layers=6,\n",
    "    heads=4,\n",
    "    dim_head=16,\n",
    "    num_classes=10,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Recon Model parameters: {sum(p.numel() for p in model_recon.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Pre-training with reconstruction (+ light classification)\n",
    "NUM_EPOCHS_PRETRAIN = 20\n",
    "LR_PRETRAIN = 1e-3\n",
    "\n",
    "optimizer_pretrain = AdamW(model_recon.parameters(), lr=LR_PRETRAIN, weight_decay=0.01)\n",
    "scheduler_pretrain = CosineAnnealingLR(optimizer_pretrain, T_max=NUM_EPOCHS_PRETRAIN, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TONIC_AVAILABLE:\n",
    "    # Phase 1: Reconstruction pre-training\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Phase 1: Reconstruction Pre-training\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    history_pretrain = {'loss': [], 'recon_loss': [], 'cls_loss': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(1, NUM_EPOCHS_PRETRAIN + 1):\n",
    "        loss, recon_loss, cls_loss, acc = train_epoch_recon(\n",
    "            model_recon, train_loader, optimizer_pretrain, DEVICE, epoch,\n",
    "            recon_weight=1.0, cls_weight=0.1\n",
    "        )\n",
    "        scheduler_pretrain.step()\n",
    "        \n",
    "        history_pretrain['loss'].append(loss)\n",
    "        history_pretrain['recon_loss'].append(recon_loss)\n",
    "        history_pretrain['cls_loss'].append(cls_loss)\n",
    "        history_pretrain['acc'].append(acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Loss={loss:.4f}, Recon={recon_loss:.4f}, Cls={cls_loss:.4f}, Acc={acc:.2f}%\")\n",
    "    \n",
    "    # Save pre-trained model\n",
    "    torch.save(model_recon.state_dict(), 'omnifield_nmnist_pretrained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Fine-tuning for classification\n",
    "NUM_EPOCHS_FINETUNE = 15\n",
    "LR_FINETUNE = 5e-4\n",
    "\n",
    "# Optionally freeze encoder and only train classification head\n",
    "# for param in model_recon.icmr_blocks.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "optimizer_finetune = AdamW(model_recon.parameters(), lr=LR_FINETUNE, weight_decay=0.01)\n",
    "scheduler_finetune = CosineAnnealingLR(optimizer_finetune, T_max=NUM_EPOCHS_FINETUNE, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TONIC_AVAILABLE:\n",
    "    # Phase 2: Fine-tuning\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Phase 2: Fine-tuning for Classification\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    history_finetune = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_acc_ft = 0\n",
    "    \n",
    "    for epoch in range(1, NUM_EPOCHS_FINETUNE + 1):\n",
    "        train_loss, train_acc = train_epoch_e2e(model_recon, train_loader, optimizer_finetune, DEVICE, epoch)\n",
    "        val_loss, val_acc, _, _ = evaluate(model_recon, test_loader, DEVICE)\n",
    "        scheduler_finetune.step()\n",
    "        \n",
    "        history_finetune['train_loss'].append(train_loss)\n",
    "        history_finetune['train_acc'].append(train_acc)\n",
    "        history_finetune['val_loss'].append(val_loss)\n",
    "        history_finetune['val_acc'].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc_ft:\n",
    "            best_val_acc_ft = val_acc\n",
    "            torch.save(model_recon.state_dict(), 'omnifield_nmnist_finetuned_best.pt')\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, \"\n",
    "              f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nBest Fine-tuned Validation Accuracy: {best_val_acc_ft:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TONIC_AVAILABLE and len(history_pretrain['loss']) > 0:\n",
    "    # Plot reconstruction pre-training curves\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].plot(history_pretrain['recon_loss'], label='Reconstruction')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('MSE Loss')\n",
    "    axes[0].set_title('Pre-training: Reconstruction Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    axes[1].plot(history_pretrain['cls_loss'], label='Classification', color='orange')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('CE Loss')\n",
    "    axes[1].set_title('Pre-training: Classification Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    axes[2].plot(history_pretrain['acc'], label='Accuracy', color='green')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Accuracy (%)')\n",
    "    axes[2].set_title('Pre-training: Classification Accuracy')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pretrain_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TONIC_AVAILABLE and len(history_finetune['train_loss']) > 0:\n",
    "    # Plot fine-tuning curves\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].plot(history_finetune['train_loss'], label='Train')\n",
    "    axes[0].plot(history_finetune['val_loss'], label='Val')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Fine-tuning: Classification Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    axes[1].plot(history_finetune['train_acc'], label='Train')\n",
    "    axes[1].plot(history_finetune['val_acc'], label='Val')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Fine-tuning: Classification Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('finetune_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Visualization and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TONIC_AVAILABLE:\n",
    "    # Load best models and evaluate\n",
    "    model_e2e.load_state_dict(torch.load('omnifield_nmnist_e2e_best.pt', map_location=DEVICE))\n",
    "    model_recon.load_state_dict(torch.load('omnifield_nmnist_finetuned_best.pt', map_location=DEVICE))\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Final Evaluation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    _, e2e_acc, e2e_preds, e2e_labels = evaluate(model_e2e, test_loader, DEVICE)\n",
    "    _, ft_acc, ft_preds, ft_labels, recon_samples = evaluate(model_recon, test_loader, DEVICE, return_recon=True)\n",
    "    \n",
    "    print(f\"\\nEnd-to-End Classification Accuracy: {e2e_acc:.2f}%\")\n",
    "    print(f\"Reconstruction + Fine-tuning Accuracy: {ft_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TONIC_AVAILABLE:\n",
    "    # Confusion matrices\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    cm_e2e = confusion_matrix(e2e_labels, e2e_preds)\n",
    "    cm_ft = confusion_matrix(ft_labels, ft_preds)\n",
    "    \n",
    "    sns.heatmap(cm_e2e, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('True')\n",
    "    axes[0].set_title(f'E2E Classification (Acc: {e2e_acc:.2f}%)')\n",
    "    \n",
    "    sns.heatmap(cm_ft, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('True')\n",
    "    axes[1].set_title(f'Recon + Fine-tune (Acc: {ft_acc:.2f}%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TONIC_AVAILABLE and len(recon_samples) > 0:\n",
    "    # Visualize reconstructions\n",
    "    fig, axes = plt.subplots(3, 6, figsize=(15, 8))\n",
    "    \n",
    "    sample = recon_samples[0]\n",
    "    \n",
    "    for i in range(6):\n",
    "        # Target\n",
    "        axes[0, i].imshow(sample['target'][i].numpy(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "        axes[0, i].set_title(f'Target (Label: {sample[\"label\"][i].item()})')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Reconstruction\n",
    "        recon_img = sample['recon'][i].reshape(34, 34).numpy()\n",
    "        axes[1, i].imshow(recon_img, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "        axes[1, i].set_title('Reconstruction')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Difference\n",
    "        diff = np.abs(sample['target'][i].numpy() - recon_img)\n",
    "        axes[2, i].imshow(diff, cmap='hot', vmin=0, vmax=0.5)\n",
    "        axes[2, i].set_title(f'|Error| (MSE: {diff.mean():.4f})')\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('Target', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Recon', fontsize=12)\n",
    "    axes[2, 0].set_ylabel('Error', fontsize=12)\n",
    "    \n",
    "    plt.suptitle('Reconstruction Results', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reconstruction_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "print(\"=\" * 60)\n",
    "print(\"OmniField N-MNIST Model Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  - Base dimension: 64 (small)\")\n",
    "print(f\"  - Number of latents: 64\")\n",
    "print(f\"  - ICMR layers: 6 (deep)\")\n",
    "print(f\"  - Attention heads: 4\")\n",
    "print(f\"  - Head dimension: 16\")\n",
    "print(f\"  - Total parameters: {sum(p.numel() for p in model_e2e.parameters()):,}\")\n",
    "\n",
    "print(f\"\\nPositional Encoding:\")\n",
    "print(f\"  - Spatial GFF: 2 -> 32 features\")\n",
    "print(f\"  - Temporal GFF: 1 -> 16 features\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  - E2E epochs: {NUM_EPOCHS_E2E}\")\n",
    "print(f\"  - Pre-train epochs: {NUM_EPOCHS_PRETRAIN}\")\n",
    "print(f\"  - Fine-tune epochs: {NUM_EPOCHS_FINETUNE}\")\n",
    "\n",
    "if TONIC_AVAILABLE:\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  - E2E Classification: {e2e_acc:.2f}%\")\n",
    "    print(f\"  - Recon + Fine-tune: {ft_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture diagram (text-based)\n",
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                  OmniField for N-MNIST                          ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║                                                                  ║\n",
    "║  INPUT: Sparse Events (x, y, t, polarity)                       ║\n",
    "║         └── N events per sample (~2000 avg)                     ║\n",
    "║                                                                  ║\n",
    "║  ┌─────────────────────────────────────────────────────────┐    ║\n",
    "║  │ ENCODER                                                  │    ║\n",
    "║  │  ├── Spatial GFF: (x,y) → 32-dim                        │    ║\n",
    "║  │  ├── Temporal GFF: t → 16-dim                           │    ║\n",
    "║  │  ├── Polarity: p → 1-dim                                │    ║\n",
    "║  │  └── MLP: 49 → 64-dim                                   │    ║\n",
    "║  └─────────────────────────────────────────────────────────┘    ║\n",
    "║                           ↓                                      ║\n",
    "║  ┌─────────────────────────────────────────────────────────┐    ║\n",
    "║  │ ICMR BLOCKS (×6)                                         │    ║\n",
    "║  │  ├── Sinusoidal Latents: 64 tokens × 64-dim             │    ║\n",
    "║  │  ├── Cross-Attention: latents ← events                  │    ║\n",
    "║  │  ├── Global Feature: mean(latents) → 64-dim             │    ║\n",
    "║  │  ├── Self-Attention: latents ← latents                  │    ║\n",
    "║  │  └── Feed-Forward: GEGLU                                │    ║\n",
    "║  │                                                          │    ║\n",
    "║  │  [Iterative Refinement via Global Feature z]            │    ║\n",
    "║  └─────────────────────────────────────────────────────────┘    ║\n",
    "║                           ↓                                      ║\n",
    "║  ┌─────────────────┐     ┌─────────────────────────────────┐    ║\n",
    "║  │ CLASSIFICATION  │     │ RECONSTRUCTION                   │    ║\n",
    "║  │  └── Global     │     │  ├── Query Grid: 34×34          │    ║\n",
    "║  │      Feature    │     │  ├── Cross-Attn: queries←latents│    ║\n",
    "║  │      → MLP      │     │  └── MLP → intensity            │    ║\n",
    "║  │      → 10 cls   │     │                                  │    ║\n",
    "║  └─────────────────┘     └─────────────────────────────────┘    ║\n",
    "║                                                                  ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "### Key Design Decisions:\n",
    "\n",
    "1. **Small Base Model (dim=64)**: Keeps the model lightweight while still expressive\n",
    "\n",
    "2. **Deep ICMR (6 layers)**: Allows for iterative refinement of the global representation\n",
    "   - Each ICMR block refines latents using cross-attention to events\n",
    "   - Global feature z is updated via mean pooling and passed to next layer\n",
    "   - This enables progressive alignment of sparse event information\n",
    "\n",
    "3. **Gaussian Fourier Features**: Better high-frequency learning than standard positional encoding\n",
    "\n",
    "4. **Dual Task Training**: \n",
    "   - Reconstruction pretraining helps learn better spatial representations\n",
    "   - Fine-tuning focuses the model on classification\n",
    "\n",
    "### Expected Results:\n",
    "- N-MNIST is relatively easy (typical accuracy >95%)\n",
    "- The reconstruction + fine-tuning approach may provide marginal improvements\n",
    "- The neural field approach naturally handles the sparse, irregular event data\n",
    "\n",
    "### Future Improvements:\n",
    "- Add temporal attention for better handling of event sequences\n",
    "- Experiment with different ICMR depths and latent sizes\n",
    "- Try harder datasets like N-Caltech101 or DVS-CIFAR10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
